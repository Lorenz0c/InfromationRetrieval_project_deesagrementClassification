{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ir_project_deesagreement_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ufdwPEY7YR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50b2c6d-7c7b-4e94-f7bc-91f0d49ef225"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from urllib.request import urlopen\n",
        "import tarfile\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import json\n",
        "\n",
        "\n",
        "import xml.etree.ElementTree as et\n",
        "import os\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import adjusted_rand_score, recall_score, precision_score\n",
        "from sklearn.metrics import plot_confusion_matrix, f1_score, classification_report\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAa8gpC8ANF_"
      },
      "source": [
        "**Labels for the two dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LwfI97i-1i2"
      },
      "source": [
        "# Possible values for the labels varies from 0 to 6, each one identify one of the 7 level of disagreement indicated by Paul Graham.\n",
        "\n",
        "# Ordered label for the dataset from the dabate forum (create_debate) (0-1570 entries (1571 total)).\n",
        "# Of 1571 value 1000 are not \"None\".\n",
        "\n",
        "y_forum=[None,0,4,None,0,3,4,2,0,0,6,None,3,1,None,4,5,0,1,4,3,4,4,None,3,6,3,5,3,4,3,5,2,None,4,2,3,3,None,4,4,3,4,3,3,4,3,None,2,2,0,2,4,4,4,5,4,4,3,3,3,4,5,5,4,6,3,None,2,3,None,None,2,4,3,6,3,4,None,None,None,None,5,None,None,0,4,6,2,3,6,5,2,2,4,None,5,0,6,3,2,0,None,None,5,3,4,3,0,3,3,3,4,3,4,3,None,2,None,3,3,None,None,6,0,0,4,1,None,None,0,4,3,4,5,3,3,5,3,5,5,5,5,5,3,4,5,3,4,2,4,3,5,4,5,6,4,4,5,4,4,4,4,4,2,4,None,4,4,2,2,4,4,3,2,2,None,None,3,6,4,0,5,None,0,4,0,4,4,4,5,2,4,4,4,4,None,6,4,4,3,4,4,3,4,4,4,4,4,4,4,4,3,4,4,4,4,4,6,4,3,4,3,4,3,4,2,3,2,3,5,3,5,4,5,5,2,5,6,3,6,4,4,2,4,None,None,None,6,None,None,3,3,2,2,3,3,2,None,None,None,None,2,None,0,6,2,0,0,3,5,3,4,4,2,4,3,3,5,4,3,3,2,4,0,0,None,None,4,None,4,5,None,3,4,4,4,2,3,0,3,4,6,4,2,4,3,3,4,3,4,3,3,3,5,5,5,3,3,4,4,4,6,4,4,1,3,3,4,2,None,None,4,4,None,4,2,0,0,3,None,2,5,4,4,None,None,None,3,4,6,0,4,6,3,4,3,6,4,6,5,4,4,4,4,3,4,4,4,5,4,None,None,None,5,4,3,None,4,4,4,None,5,4,None,None,None,3,0,3,4,4,4,4,None,None,4,0,3,4,0,None,4,4,4,2,None,3,3,5,3,3,1,2,2,4,4,3,3,3,None,3,3,4,4,4,3,5,None,6,3,4,4,4,3,2,3,3,3,3,6,3,4,3,4,4,3,3,4,0,5,6,None,4,None,3,3,None,4,3,3,3,None,4,4,4,4,3,2,4,3,3,4,5,4,4,None,4,5,None,None,None,None,None,None,None,None,None,None,None,3,None,3,None,4,5,3,3,2,4,4,6,4,2,4,5,2,4,4,4,6,2,4,5,6,None,2,2,0,0,5,2,3,4,5,3,4,4,4,5,5,6,5,2,4,5,4,5,4,3,None,None,5,2,4,None,3,3,None,2,4,3,3,3,4,4,3,4,None,2,3,4,5,4,4,2,2,4,5,5,3,3,5,4,None,4,None,None,None,4,0,None,None,0,2,None,6,None,4,None,2,4,4,4,4,4,None,4,4,4,4,4,4,3,None,4,4,3,2,0,None,None,3,3,None,4,None,3,3,4,4,2,4,None,0,4,2,5,6,None,3,4,4,4,6,5,3,0,4,4,6,3,4,3,3,3,5,3,3,5,3,4,0,4,2,0,None,2,2,2,None,3,3,3,2,4,1,4,4,None,6,3,0,6,3,3,3,3,3,4,3,4,3,2,3,4,None,4,2,None,6,2,0,0,3,None,0,4,4,4,2,4,0,3,None,0,2,6,4,4,0,None,None,None,None,None,None,None,4,4,4,None,None,0,None,4,3,4,None,None,3,None,None,2,3,4,4,None,None,None,4,0,3,3,6,None,4,3,0,3,2,None,4,4,4,4,3,0,3,4,4,4,4,3,4,4,4,6,4,4,4,3,4,None,None,None,4,None,None,None,None,None,None,None,None,None,None,None,None,2,None,None,None,None,None,None,None,None,None,None,None,None,None,None,4,3,6,4,0,2,5,4,4,3,3,4,6,4,4,4,3,5,4,3,0,4,3,4,4,4,1,3,None,1,0,5,0,0,2,3,5,6,3,2,None,None,5,2,None,4,4,4,4,4,4,3,4,3,4,3,6,4,3,4,4,4,4,4,4,2,6,None,6,6,4,4,6,3,4,3,6,3,0,4,None,4,4,4,4,2,2,3,6,4,5,4,4,3,2,3,None,4,6,4,2,4,4,4,4,4,6,None,None,None,None,None,None,5,4,4,4,4,4,4,5,3,4,3,3,3,2,6,3,4,4,6,None,4,4,4,4,4,4,6,3,4,4,4,4,3,3,4,3,3,3,4,3,None,None,None,None,3,6,None,None,None,None,None,2,3,None,2,6,4,4,4,4,None,None,4,4,6,3,6,3,None,None,None,None,None,None,3,4,4,4,4,4,None,4,3,3,None,3,6,4,4,4,3,4,3,3,3,4,3,4,4,4,4,4,4,2,2,None,4,None,3,None,4,2,4,4,4,4,None,None,4,None,3,4,3,4,4,2,2,4,3,None,3,4,4,4,3,4,3,None,3,0,None,None,None,0,None,0,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,1,None,None,None,3,None,None,4,None,6,4,None,None,None,None,None,None,None,None,None,None,None,None,0,2,None,None,None,4,4,2,3,4,6,4,4,4,4,4,3,5,4,0,3,0,0,3,3,4,5,5,4,None,1,4,3,4,4,3,3,2,3,2,4,4,3,2,4,4,6,5,3,None,3,3,3,3,3,3,4,3,3,3,2,5,None,0,None,3,3,3,3,5,5,4,4,4,3,4,3,3,4,4,4,4,None,4,3,4,4,2,4,None,3,3,4,4,3,4,None,4,4,6,3,4,4,6,3,4,6,4,None,4,5,5,4,4,6,4,3,None,None,None,None,None,4,4,4,2,0,4,4,3,4,5,4,3,4,4,4,3,3,4,4,4,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,3,None,4,4,4,5,None,3,4,4,4,4,4,4,4,5]\n",
        "\n",
        "\n",
        "# Ordered label for the dataset from chat (sashank) (0-210 entries (211 total)).\n",
        "# Of 209 value 200 are not \"None\".\n",
        "\n",
        "y_chat=[4,3,4,4,3,4,3,4,4,3,4,4,4,5,3,4,4,3,None,3,3,4,3,2,3,5,6,5,5,5,5,4,4,4,5,4,4,4,3,3,3,3,4,4,4,5,4,4,4,3,3,3,2,3,4,3,3,3,4,5,2,4,3,3,3,3,3,3,3,3,6,4,4,3,3,3,None,4,None,None,4,3,3,3,4,3,3,4,4,4,4,3,4,4,4,4,4,4,4,4,4,4,4,None,4,3,4,3,4,4,4,2,4,4,4,4,3,4,None,4,4,3,4,4,None,4,3,4,2,3,4,6,3,4,3,3,None,4,4,4,4,4,3,5,4,3,4,4,4,4,4,4,4,4,4,3,4,None,3,3,3,3,3,6,5,4,None,4,3,4,4,3,3,4,4,3,6,6,3,4,4,4,3,4,4,4,4,4,3,4,4,4,4,4,4,4,4,3,4,3,4,3,3,4,4,3,4,3,4,None,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "oPV2hZmnAuiP",
        "outputId": "54067f62-78da-4cc1-ccab-349d59ae9485"
      },
      "source": [
        "# Count the number of occurence of each label in the forum dataset.\n",
        "count_forum=Counter(y_forum)\n",
        "for i in range(7):\n",
        "  if i not in count_forum:\n",
        "    count_forum[i]=0\n",
        "\n",
        "labels = []\n",
        "counts = []\n",
        "for key,value in list(count_forum.items()):\n",
        "  if key is not None:\n",
        "    counts.append(value)\n",
        "    labels.append(key)\n",
        "plt.bar(labels, counts)\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "4-6_bGxiKJOY",
        "outputId": "53fd8c2f-d058-4a2d-c431-80fee0b084e6"
      },
      "source": [
        "# Count the number of occurence of each label in the chat dataset.\n",
        "count_chat=Counter(y_chat)\n",
        "for i in range(7):\n",
        "  if i not in count_chat:\n",
        "    count_chat[i]=0\n",
        "\n",
        "labels = []\n",
        "counts = []\n",
        "for key,value in list(count_chat.items()):\n",
        "    if key is not None:\n",
        "      counts.append(value)\n",
        "      labels.append(key)\n",
        "plt.bar(labels, counts)\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKCcusa6QJm3"
      },
      "source": [
        "**Download of the two dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyQAjVx7ZBq_"
      },
      "source": [
        "# Download and unzip the forum dataset (tgz file).\n",
        "\n",
        "url_forum_dataset=\"http://www.cs.columbia.edu/~sara/download/create_debate.tgz\"\n",
        "\n",
        "forum_dataset_path = \"debate_dataset\"\n",
        "\n",
        "unzipped = requests.get(url_forum_dataset, allow_redirects=True)\n",
        "\n",
        "open('unzipped_dataset', 'wb').write(unzipped.content)\n",
        "\n",
        "tar = tarfile.open('unzipped_dataset', \"r:gz\")\n",
        "tar.extractall(path=forum_dataset_path)\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uUYk85W9vzW"
      },
      "source": [
        "# Download and unzip the chat dataset (zip file).\n",
        "\n",
        "url_sashank = 'https://github.com/sashank06/MPC-Corpus/raw/master/MPC%20CORPUS.zip'\n",
        "\n",
        "sashank_path=\"sashank\"\n",
        "\n",
        "zipped_file = ZipFile(BytesIO(urlopen(url_sashank).read()))\n",
        "zipped_file.extractall(path=sashank_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSAom8GODMg"
      },
      "source": [
        "**Create two dictionaries containing *all* the messages of the two dataset** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yBmjkHVeK4l",
        "outputId": "d8f18b5e-67a5-4426-ed07-99299fd2273e"
      },
      "source": [
        "# For the forum dataset:\n",
        "\n",
        "# Create a dictionary document_messages_forum of all the message in the files contained in the folder.\n",
        "# The dictionary contain another dictionary for each file in the folder.\n",
        "# This inner dictionary contains another dictionary with two entries: one for the text of the \n",
        "# message (text), the other for the side of the message in the conversation (side).\n",
        "\n",
        "folder_name=\"/content/debate_dataset/create_debate/training\"\n",
        "\n",
        "document_messages_forum={}\n",
        "\n",
        "for file in os.listdir(folder_name):\n",
        "  # Create the inner dictionary.\n",
        "  document_messages_forum[file]={}\n",
        "\n",
        "  try:\n",
        "    xml_tree=et.parse(folder_name+\"/\"+file)\n",
        "    xml_root=xml_tree.getroot()\n",
        "\n",
        "    # Next line is strictly correlated to the structure of the XML files used.\n",
        "    for node in xml_root.findall(\".//item//comments//comment\"):\n",
        "      # Get the text of the message.\n",
        "      message=node.find(\"text\")\n",
        "      # Get the id (url field) of the message.\n",
        "      url=node.attrib.get(\"url\")\n",
        "      # Create most inner the dictionary.\n",
        "      document_messages_forum[file][url]={\"text\":message.text,\"side\":node.attrib.get(\"side\")}\n",
        "  \n",
        "  except:\n",
        "    print(file+\" not selected due to an invalid character contained in the file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debate.show.Does_history_have_value_outside_of_academics_lj.xml not selected due to an invalid character contained in the file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2vg1uhRYxZc"
      },
      "source": [
        "# For the chat dataset:\n",
        "\n",
        "# Create a dictionary document_messages_chat of all the message in the files contained in the folder.\n",
        "# The dictionary contain another dictionary for each file in the folder.\n",
        "# This second dictionary associates the text of the message with its identifier (turn_no).\n",
        "\n",
        "folder_name=\"/content/sashank/MPC CORPUS/chat_sessions_annotated\"\n",
        "\n",
        "document_messages_chat={}\n",
        "\n",
        "for file in os.listdir(folder_name):\n",
        "  document_messages_chat[file]={}\n",
        "  xml_tree=et.parse(folder_name+\"/\"+file)\n",
        "  xml_root=xml_tree.getroot()\n",
        "\n",
        "  for node in xml_root:\n",
        "    turn=node.attrib.get(\"turn_no\")\n",
        "    document_messages_chat[file][turn]=node.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBxg2thaS4R9"
      },
      "source": [
        "**Create a list of the messages to classify**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS3OQZ1cTUiM",
        "outputId": "cfd66f70-1fec-4137-abc9-d6ac982abe9a"
      },
      "source": [
        "# For the forum dataset:\n",
        "\n",
        "# Create a list containing all the disagreement messages and the message they are responding to.\n",
        "# Even positions will contain the messages, the following odd position the message they are referring to. \n",
        "\n",
        "folder_name=\"/content/debate_dataset/create_debate/training\"\n",
        "\n",
        "rows_forum=[]\n",
        "\n",
        "for file in sorted(os.listdir(folder_name)):\n",
        "  try:\n",
        "    xml_tree=et.parse(folder_name+\"/\"+file)\n",
        "    xml_root=xml_tree.getroot()\n",
        "\n",
        "    for node in xml_root.findall(\".//item//comments//comment\"):\n",
        "      # Link to the referring message (the one the response desagree with).\n",
        "      link=node.attrib.get(\"parent-url\")\n",
        "      side=node.attrib.get(\"side\")\n",
        "      url=node.attrib.get(\"url\")\n",
        "\n",
        "      # Check that the message is a response to another message (parent-url different from -1), \n",
        "      # and that the response support a different opinion (side) with respect to the message it responds to.\n",
        "      if (link!=\"-1\" and side!=document_messages_forum[file][link][\"side\"]):\n",
        "      \n",
        "        # Get the text of the message.\n",
        "        message=node.find(\"text\")\n",
        "\n",
        "        # Add the original message and the response in the list of disagreement response.\n",
        "        rows_forum.append(message.text)\n",
        "        rows_forum.append(document_messages_forum[file][link][\"text\"])\n",
        "\n",
        "  except:\n",
        "    print(file+\" not selected due to an invalid character contained in the file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debate.show.Does_history_have_value_outside_of_academics_lj.xml not selected due to an invalid character contained in the file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l9WFoQRvlvI"
      },
      "source": [
        "# For the chat dataset:\n",
        "\n",
        "# Create a list containing all the disagreement messages and the message they are responding to.\n",
        "# Even positions will contain the messages, the following odd position the message they are referring to. \n",
        "\n",
        "# In the first step each messages is stored in a dictionary with the speaker and the message they are directed to.\n",
        "# This dictionaries are stored in the list rows.\n",
        "\n",
        "folder_name=\"/content/sashank/MPC CORPUS/chat_sessions_annotated\"\n",
        "\n",
        "rows=[]\n",
        "\n",
        "for file in sorted(os.listdir(folder_name)):\n",
        "  xml_tree=et.parse(folder_name+\"/\"+file)\n",
        "  xml_root=xml_tree.getroot()\n",
        "\n",
        "  first_line=True\n",
        "  # A dictionary will contain, for each speaker, the last line written.\n",
        "  last_line_speaked={}\n",
        "\n",
        "  for node in xml_root:\n",
        "    # Who has written the message.\n",
        "    speaker=node.attrib.get(\"speaker\")\n",
        "    # Link to the referring message (the one the speaker desagree with).\n",
        "    link=node.attrib.get(\"link_to\")\n",
        "\n",
        "    # Check that the line is not the first of the file, and that the line is a desagreement\n",
        "    if ((node.attrib.get(\"dialog_act\")==\"TASK:--Disagree-Reject\") and (not (first_line))):\n",
        "      \n",
        "      # Get the line of the message the disagreement refers (linked) to (format of link is name_user:line).\n",
        "      # Not all the links to these messages contains the number of the line, some contains only the name of the speaker (refers to the last message witten by him/her).\n",
        "      if (\":\" in link):\n",
        "        _,line_message=link.split(\":\")\n",
        "        line=document_messages_chat[file][line_message]\n",
        "      else:\n",
        "        line=last_line_speaked[speaker]\n",
        "\n",
        "      rows.append({\"opinion_expression\":line,\"response\":node.text, \"speaker\":speaker,\"file\":file})  \n",
        "\n",
        "    first_line=False\n",
        "    # Last line witten by the speaker.\n",
        "    last_line_speaked[speaker]=node.text\n",
        "\n",
        "\n",
        "\n",
        "# Combine together the messages that are divide in differents chat lines.\n",
        "# When modifiing a response store the old one to check if it match the next message opinion_expression.\n",
        "rows_to_eliminate=[]\n",
        "old_response=None\n",
        "\n",
        "for i in range(0,len(rows)-1):\n",
        "  if ((rows[i][\"response\"]==rows[i+1][\"opinion_expression\"] or old_response==rows[i+1][\"opinion_expression\"])and rows[i][\"speaker\"]==rows[i+1][\"speaker\"]):\n",
        "    rows_to_eliminate.append(rows[i])\n",
        "    rows[i+1][\"opinion_expression\"]=rows[i][\"opinion_expression\"]\n",
        "    old_response=rows[i+1][\"response\"]\n",
        "    rows[i+1][\"response\"]=rows[i][\"response\"]+\" \"+rows[i+1][\"response\"]\n",
        "\n",
        "for row in rows_to_eliminate:\n",
        "  rows.remove(row)\n",
        "\n",
        "# Eliminate the repeating messages and responses.\n",
        "rows_to_eliminate=[]\n",
        "\n",
        "for i in range(0,len(rows)):\n",
        "  for j in range((i+1),len(rows)):\n",
        "    if (rows[i][\"response\"]==rows[j][\"response\"] and rows[i][\"opinion_expression\"]==rows[j][\"opinion_expression\"]):\n",
        "      rows_to_eliminate.append(rows[j])\n",
        "\n",
        "for row in rows_to_eliminate:\n",
        "  rows.remove(row)\n",
        "\n",
        "\n",
        "\n",
        "# Create the final list.\n",
        "rows_chat=[]\n",
        "for messages in rows:\n",
        "  rows_chat.append(messages[\"response\"])\n",
        "  rows_chat.append(messages[\"opinion_expression\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXmpqtPYWnS"
      },
      "source": [
        "**Remove the not labeled document from the lists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-UiSYoYioo"
      },
      "source": [
        "corpus_forum=[]\n",
        "corpus_chat=[]\n",
        "res=[]\n",
        "\n",
        "for i in range(len(y_forum)):\n",
        "  if y_forum[i] is not None:\n",
        "    corpus_forum.append(rows_forum[2*i])\n",
        "    corpus_forum.append(rows_forum[2*i+1])\n",
        "    res.append(y_forum[i])\n",
        "\n",
        "y_forum=res    \n",
        "res=[]\n",
        "\n",
        "for i in range(len(y_chat)):\n",
        "  if y_chat[i] is not None:\n",
        "    corpus_chat.append(rows_chat[2*i])\n",
        "    corpus_chat.append(rows_chat[2*i+1])\n",
        "    res.append(y_chat[i])\n",
        "\n",
        "y_chat=res      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "HqnZZ-Evnv2m",
        "outputId": "27db4bb6-d52c-4c2f-fc45-086509ca99a3"
      },
      "source": [
        "# Count the number of occurence of each label in the chat dataset.\n",
        "fold_y=[y_forum[0:200],y_forum[200:400],y_forum[400:600],y_forum[600:800],y_forum[800:1000]]\n",
        "colour=['r','b','g','y','brown']\n",
        "count_fold_y={}\n",
        "count={}\n",
        "\n",
        "for i in range(5):\n",
        "  count_fold_y[i]=Counter(fold_y[i])\n",
        "  for j in range(7):\n",
        "    if j not in count_fold_y[i]:\n",
        "      count_forum[i][j]=0\n",
        "\n",
        "labels = {}\n",
        "counts = {}\n",
        "for i in range(5):\n",
        "  counts[i]=[]\n",
        "  labels[i]=[]\n",
        "  for key,value in list(count_fold_y[i].items()):\n",
        "    if key is not None:\n",
        "      counts[i].append(value)\n",
        "      labels[i].append((key+0.16*i)-0.25)    \n",
        "  plt.bar(labels[i], counts[i], color = colour[i], width = 0.14)\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.legend(labels=['fold 1', 'fold 2','fold 3', 'fold 4','fold 5'])\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFO_GWaeZfPc"
      },
      "source": [
        "**Definition of the tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyopnihKZpON"
      },
      "source": [
        "#  Lemmatization is used.\n",
        "def build_tokenizer(text):\n",
        "\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  ignore_tokens = ['%s', '|', '\\\\', '~', '(', ')', '[', ']', '{', '}', '/', '\"', \"'\", '^', '\\n']\n",
        "\n",
        "  # Remove the \"<i>\",\"</i>\" (italics text style),\"<b>\",\"</b>\" (bold text style) and link(<a _ > _ </a>) occurrences.\n",
        "  # A \"<b>_</b>\" is sometimes used as quoting; Inner quotes are present as \"<b> previous_qoting </b> last_quoting </b>\".\n",
        "  # The inner quotes are eliminated and only the words of the last quoted message are keeped.\n",
        "\n",
        "  text=re.sub(r\"<\\/?i>\",\"\",text)\n",
        "  text=re.sub(r\"<\\/?u>\",\"\",text)\n",
        "  text=re.sub(r\"<\\/a>\",\"\",text)\n",
        "  text=re.sub(r\"<a href.*>\",\"\",text)\n",
        "\n",
        "  while re.search(r\"<b>.*<\\/b>.*<\\/b>\",text) is not None:\n",
        "    text=re.sub(r\"<b>.*<\\/b>\",\"<b>\",text)\n",
        "\n",
        "  text=re.sub(r\"<\\/?b>\",\"\",text)  \n",
        "\n",
        "  # Trasform to lowrcase.\n",
        "  text=text.lower()\n",
        "\n",
        "  # Text tokenization.\n",
        "\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "\n",
        "  # Text normalization throgh lemmatizzation.\n",
        "\n",
        "  lemmas=[]\n",
        "  for word in tokens:\n",
        "    if word not in ignore_tokens:\n",
        "      lemmas.append(wordnet_lemmatizer.lemmatize(word))\n",
        "\n",
        "  return lemmas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p1WgjTTZ5W8"
      },
      "source": [
        "**Datasets definition (only the forum dataset)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THIBj2A3XI98"
      },
      "source": [
        "**Bag of words Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Htn6GsZx9x"
      },
      "source": [
        "# Trasformation of the corpus in the vector space.\n",
        "vectorizer=TfidfVectorizer(tokenizer=build_tokenizer)\n",
        "bag_words_forum=vectorizer.fit_transform(corpus_forum)\n",
        "\n",
        "bag_words_forum=bag_words_forum.toarray()\n",
        "\n",
        "\n",
        "# The response_vector will contain the list of all the vector rapresentation of the response messages.\n",
        "# The original_sentence_vector will contain the list of all the vector rapresentation of the messages the responses were directed to.\n",
        "response_vector=[]\n",
        "original_sentence_vector=[]\n",
        "\n",
        "# Separate the vectros of the actual response messages and of the messages they respond to into two lists (1000 response messages).\n",
        "for i in range(1000):\n",
        "  response_vector.append(bag_words_forum[i*2])\n",
        "  original_sentence_vector.append(bag_words_forum[i*2+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "o3keQdL4URx3",
        "outputId": "a8e750c4-0820-4e89-9a49-91c2031c6650"
      },
      "source": [
        "# Dataset containing the vector rappresentation of all the labeled response messages in the forum dataset.\n",
        "X_vector_forum_dataset=pd.DataFrame(response_vector)\n",
        "\n",
        "X_vector_forum_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>8225</th>\n",
              "      <th>8226</th>\n",
              "      <th>8227</th>\n",
              "      <th>8228</th>\n",
              "      <th>8229</th>\n",
              "      <th>8230</th>\n",
              "      <th>8231</th>\n",
              "      <th>8232</th>\n",
              "      <th>8233</th>\n",
              "      <th>8234</th>\n",
              "      <th>8235</th>\n",
              "      <th>8236</th>\n",
              "      <th>8237</th>\n",
              "      <th>8238</th>\n",
              "      <th>8239</th>\n",
              "      <th>8240</th>\n",
              "      <th>8241</th>\n",
              "      <th>8242</th>\n",
              "      <th>8243</th>\n",
              "      <th>8244</th>\n",
              "      <th>8245</th>\n",
              "      <th>8246</th>\n",
              "      <th>8247</th>\n",
              "      <th>8248</th>\n",
              "      <th>8249</th>\n",
              "      <th>8250</th>\n",
              "      <th>8251</th>\n",
              "      <th>8252</th>\n",
              "      <th>8253</th>\n",
              "      <th>8254</th>\n",
              "      <th>8255</th>\n",
              "      <th>8256</th>\n",
              "      <th>8257</th>\n",
              "      <th>8258</th>\n",
              "      <th>8259</th>\n",
              "      <th>8260</th>\n",
              "      <th>8261</th>\n",
              "      <th>8262</th>\n",
              "      <th>8263</th>\n",
              "      <th>8264</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135610</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058596</td>\n",
              "      <td>0.325607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.310349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.082292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.129140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  8265 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1     2     3     4     ...  8260  8261  8262  8263  8264\n",
              "0  0.000000  0.000000   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "1  0.058596  0.325607   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "2  0.310349  0.000000   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "3  0.000000  0.000000   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "4  0.129140  0.000000   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 8265 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0Lo507XSSp"
      },
      "source": [
        "**Words embedding Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvOzdIhOl901"
      },
      "source": [
        "# Create the vocabulary for the embedding of the words (word2vec).\n",
        "word2vec_sentences=[]\n",
        "rows=rows_forum+rows_chat\n",
        "\n",
        "for row in rows:\n",
        "  word2vec_sentences.append(build_tokenizer(row))\n",
        "\n",
        "word2vec_model = Word2Vec(word2vec_sentences, min_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLeUJllumgK-"
      },
      "source": [
        "# Each disagreement message is rappresented as the medium point of the embedding of the words that compose it.\n",
        "# The message the disagreement is referred to is used to copute the difference between the disareement response and this original message. \n",
        "def embed_corpus(corpus):\n",
        "  documents_vector_rapresentation=[]\n",
        "\n",
        "  for i in range(0, len(corpus)):\n",
        "    vector_rapresentation=None\n",
        "    sentence=build_tokenizer(corpus[i])\n",
        "    counter=0\n",
        "\n",
        "    # Compute the medium point of the vectors that compose the sentence\n",
        "    for word in sentence:\n",
        "      # Check if the word is present in the vocabulary.\n",
        "      if word in word2vec_model.wv.vocab:\n",
        "        counter=counter+1\n",
        "        if vector_rapresentation is not None:\n",
        "          vector_rapresentation=vector_rapresentation+word2vec_model.wv[word]\n",
        "        else:\n",
        "          vector_rapresentation=word2vec_model.wv[word]\n",
        "\n",
        "    try:\n",
        "      vector_rapresentation=np.divide(vector_rapresentation,counter) \n",
        "      if (i %2 == 0) :   \n",
        "        documents_vector_rapresentation.append(vector_rapresentation)\n",
        "      else:\n",
        "        previous_vector=documents_vector_rapresentation[-1]\n",
        "        documents_vector_rapresentation.append(vector_rapresentation-previous_vector)\n",
        "    except:\n",
        "      print(\"A string that dosen't contain any 'comon' word as been found in position {}: {}\".format(str(i),sentence))\n",
        "\n",
        "  return documents_vector_rapresentation      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjvNu4ENWpl7"
      },
      "source": [
        "# The response_embedding will contain the list of all the embedding of the disagreement messages.\n",
        "response_embedding=[]\n",
        "\n",
        "# The original_and_response_sentence_embedding will contain the list of elements composed by both the embedding of the disagreement messages\n",
        "# and the embedding of the messages they were directed to.\n",
        "original_and_response_embedding=[]\n",
        "\n",
        "# The combined_list will contain a list of element composed by the elements composed by the embedding of the disagreement messages,\n",
        "# the embedding of the messages they were directed to, and the bag of words vector rappresentation of the disagreement messages.\n",
        "combined_list=[]\n",
        "\n",
        "documents_vector_rapresentation=embed_corpus(corpus_forum)\n",
        "\n",
        "# Separate the embedding of the actual response messages and of the messages they respond to into two lists (1000 response messages).\n",
        "for i in range(1000):\n",
        "  response_embedding.append(documents_vector_rapresentation[i*2])\n",
        "  next_vector=np.concatenate((documents_vector_rapresentation[i*2],documents_vector_rapresentation[i*2+1]),axis=None)\n",
        "  original_and_response_embedding.append(next_vector)\n",
        "  next_vector=np.concatenate((next_vector,bag_words_forum[i*2]),axis=None)\n",
        "  combined_list.append(next_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKBB-7pzXYqg"
      },
      "source": [
        "# Three dataset will be created."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "S3Of8uDpX1yX",
        "outputId": "b35efb24-46cd-428d-f1d7-b8be0e119c46"
      },
      "source": [
        "# The first will contain only the embedding of the responses.\n",
        "X_embedding_1_forum_dataset=pd.DataFrame(response_embedding)\n",
        "\n",
        "X_embedding_1_forum_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351841</td>\n",
              "      <td>0.350937</td>\n",
              "      <td>0.328896</td>\n",
              "      <td>0.188909</td>\n",
              "      <td>-0.705771</td>\n",
              "      <td>-0.108164</td>\n",
              "      <td>0.169885</td>\n",
              "      <td>0.651574</td>\n",
              "      <td>-0.232430</td>\n",
              "      <td>-0.057164</td>\n",
              "      <td>-0.312394</td>\n",
              "      <td>-0.081075</td>\n",
              "      <td>0.933955</td>\n",
              "      <td>0.850431</td>\n",
              "      <td>0.723808</td>\n",
              "      <td>0.961155</td>\n",
              "      <td>-0.370502</td>\n",
              "      <td>0.407380</td>\n",
              "      <td>-0.213364</td>\n",
              "      <td>-0.257225</td>\n",
              "      <td>1.045631</td>\n",
              "      <td>0.036601</td>\n",
              "      <td>-1.329234</td>\n",
              "      <td>0.147800</td>\n",
              "      <td>0.043450</td>\n",
              "      <td>-0.213277</td>\n",
              "      <td>0.171580</td>\n",
              "      <td>-0.036937</td>\n",
              "      <td>-0.014714</td>\n",
              "      <td>-0.256418</td>\n",
              "      <td>-0.444057</td>\n",
              "      <td>-0.288581</td>\n",
              "      <td>-0.016435</td>\n",
              "      <td>-0.016690</td>\n",
              "      <td>-0.651222</td>\n",
              "      <td>-0.105144</td>\n",
              "      <td>-0.123457</td>\n",
              "      <td>-0.719637</td>\n",
              "      <td>0.135821</td>\n",
              "      <td>-0.371884</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.004551</td>\n",
              "      <td>0.563383</td>\n",
              "      <td>0.119081</td>\n",
              "      <td>-0.299941</td>\n",
              "      <td>0.266180</td>\n",
              "      <td>-0.046452</td>\n",
              "      <td>-0.744068</td>\n",
              "      <td>-0.014914</td>\n",
              "      <td>-0.445351</td>\n",
              "      <td>-0.544169</td>\n",
              "      <td>0.929951</td>\n",
              "      <td>-0.472000</td>\n",
              "      <td>0.116934</td>\n",
              "      <td>0.871178</td>\n",
              "      <td>-0.484978</td>\n",
              "      <td>1.245497</td>\n",
              "      <td>0.265839</td>\n",
              "      <td>-0.853972</td>\n",
              "      <td>-0.808234</td>\n",
              "      <td>0.058854</td>\n",
              "      <td>0.443155</td>\n",
              "      <td>1.081247</td>\n",
              "      <td>-0.170370</td>\n",
              "      <td>-0.235817</td>\n",
              "      <td>0.182592</td>\n",
              "      <td>0.243057</td>\n",
              "      <td>0.252452</td>\n",
              "      <td>-0.340271</td>\n",
              "      <td>-0.139402</td>\n",
              "      <td>-0.351121</td>\n",
              "      <td>-0.233478</td>\n",
              "      <td>0.977568</td>\n",
              "      <td>-0.104094</td>\n",
              "      <td>0.616367</td>\n",
              "      <td>0.682930</td>\n",
              "      <td>-0.061484</td>\n",
              "      <td>0.534927</td>\n",
              "      <td>-0.834121</td>\n",
              "      <td>-0.221069</td>\n",
              "      <td>-0.150381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.690412</td>\n",
              "      <td>-0.050015</td>\n",
              "      <td>0.213738</td>\n",
              "      <td>0.477493</td>\n",
              "      <td>-0.355049</td>\n",
              "      <td>-0.184899</td>\n",
              "      <td>0.370410</td>\n",
              "      <td>0.576586</td>\n",
              "      <td>-0.028780</td>\n",
              "      <td>-0.266123</td>\n",
              "      <td>-0.594824</td>\n",
              "      <td>-0.065822</td>\n",
              "      <td>0.635375</td>\n",
              "      <td>0.495326</td>\n",
              "      <td>0.178782</td>\n",
              "      <td>0.609307</td>\n",
              "      <td>0.173820</td>\n",
              "      <td>0.369583</td>\n",
              "      <td>-0.523248</td>\n",
              "      <td>-0.345976</td>\n",
              "      <td>0.784168</td>\n",
              "      <td>0.135446</td>\n",
              "      <td>-0.725013</td>\n",
              "      <td>0.278725</td>\n",
              "      <td>-0.308128</td>\n",
              "      <td>-0.372479</td>\n",
              "      <td>0.841734</td>\n",
              "      <td>-0.183544</td>\n",
              "      <td>0.254608</td>\n",
              "      <td>0.161915</td>\n",
              "      <td>-0.415911</td>\n",
              "      <td>0.008910</td>\n",
              "      <td>-0.327533</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>-0.478026</td>\n",
              "      <td>-0.349667</td>\n",
              "      <td>-0.299333</td>\n",
              "      <td>-0.614111</td>\n",
              "      <td>-0.161426</td>\n",
              "      <td>-0.290567</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079567</td>\n",
              "      <td>-0.026334</td>\n",
              "      <td>-0.013050</td>\n",
              "      <td>-0.097387</td>\n",
              "      <td>0.097073</td>\n",
              "      <td>0.260140</td>\n",
              "      <td>-0.994156</td>\n",
              "      <td>0.247306</td>\n",
              "      <td>-0.217401</td>\n",
              "      <td>-0.402763</td>\n",
              "      <td>0.854217</td>\n",
              "      <td>-0.283722</td>\n",
              "      <td>0.428157</td>\n",
              "      <td>0.726902</td>\n",
              "      <td>-0.375662</td>\n",
              "      <td>0.592772</td>\n",
              "      <td>-0.059458</td>\n",
              "      <td>-0.658963</td>\n",
              "      <td>0.206312</td>\n",
              "      <td>-0.087451</td>\n",
              "      <td>0.119125</td>\n",
              "      <td>0.583178</td>\n",
              "      <td>0.059920</td>\n",
              "      <td>-0.163893</td>\n",
              "      <td>0.178832</td>\n",
              "      <td>-0.241985</td>\n",
              "      <td>0.428290</td>\n",
              "      <td>-0.309635</td>\n",
              "      <td>-0.570607</td>\n",
              "      <td>-0.247399</td>\n",
              "      <td>-0.017832</td>\n",
              "      <td>0.446050</td>\n",
              "      <td>-0.138135</td>\n",
              "      <td>0.520635</td>\n",
              "      <td>0.621071</td>\n",
              "      <td>0.062469</td>\n",
              "      <td>0.084696</td>\n",
              "      <td>-0.711145</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>-0.138732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.878922</td>\n",
              "      <td>0.129748</td>\n",
              "      <td>0.209894</td>\n",
              "      <td>0.477275</td>\n",
              "      <td>-0.013467</td>\n",
              "      <td>-0.034873</td>\n",
              "      <td>0.166108</td>\n",
              "      <td>0.566422</td>\n",
              "      <td>-0.104887</td>\n",
              "      <td>-0.361374</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>-0.308725</td>\n",
              "      <td>0.359111</td>\n",
              "      <td>0.733559</td>\n",
              "      <td>0.259489</td>\n",
              "      <td>0.489453</td>\n",
              "      <td>-0.126814</td>\n",
              "      <td>0.459890</td>\n",
              "      <td>-0.236492</td>\n",
              "      <td>-0.534941</td>\n",
              "      <td>0.799984</td>\n",
              "      <td>0.049314</td>\n",
              "      <td>-0.792654</td>\n",
              "      <td>0.095532</td>\n",
              "      <td>-0.057992</td>\n",
              "      <td>-0.682315</td>\n",
              "      <td>0.595183</td>\n",
              "      <td>0.225203</td>\n",
              "      <td>0.173171</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.148691</td>\n",
              "      <td>0.280422</td>\n",
              "      <td>-0.013118</td>\n",
              "      <td>-0.173365</td>\n",
              "      <td>-0.611526</td>\n",
              "      <td>-0.524104</td>\n",
              "      <td>-0.158826</td>\n",
              "      <td>-0.621647</td>\n",
              "      <td>-0.106134</td>\n",
              "      <td>0.120485</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.656568</td>\n",
              "      <td>0.537230</td>\n",
              "      <td>-0.053392</td>\n",
              "      <td>-0.513315</td>\n",
              "      <td>-0.216077</td>\n",
              "      <td>-0.003642</td>\n",
              "      <td>-1.448524</td>\n",
              "      <td>-0.016003</td>\n",
              "      <td>-0.384621</td>\n",
              "      <td>-0.541307</td>\n",
              "      <td>1.137415</td>\n",
              "      <td>-0.050873</td>\n",
              "      <td>0.120627</td>\n",
              "      <td>0.724366</td>\n",
              "      <td>-0.067204</td>\n",
              "      <td>1.219086</td>\n",
              "      <td>0.090568</td>\n",
              "      <td>-0.815320</td>\n",
              "      <td>-0.030529</td>\n",
              "      <td>0.263737</td>\n",
              "      <td>0.302234</td>\n",
              "      <td>0.773223</td>\n",
              "      <td>0.283574</td>\n",
              "      <td>-0.217457</td>\n",
              "      <td>-0.276619</td>\n",
              "      <td>0.181044</td>\n",
              "      <td>0.085590</td>\n",
              "      <td>0.148161</td>\n",
              "      <td>-0.256815</td>\n",
              "      <td>-0.107639</td>\n",
              "      <td>-0.442596</td>\n",
              "      <td>0.426419</td>\n",
              "      <td>-0.612177</td>\n",
              "      <td>0.638236</td>\n",
              "      <td>0.832539</td>\n",
              "      <td>0.279702</td>\n",
              "      <td>0.458219</td>\n",
              "      <td>-0.783624</td>\n",
              "      <td>-0.406802</td>\n",
              "      <td>-0.646021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.131087</td>\n",
              "      <td>0.040849</td>\n",
              "      <td>0.268820</td>\n",
              "      <td>0.166719</td>\n",
              "      <td>-0.283283</td>\n",
              "      <td>-0.121891</td>\n",
              "      <td>0.127263</td>\n",
              "      <td>0.330898</td>\n",
              "      <td>-0.481086</td>\n",
              "      <td>0.066487</td>\n",
              "      <td>0.104336</td>\n",
              "      <td>-0.175074</td>\n",
              "      <td>0.413281</td>\n",
              "      <td>0.214315</td>\n",
              "      <td>-0.010617</td>\n",
              "      <td>0.453170</td>\n",
              "      <td>-0.272459</td>\n",
              "      <td>-0.008220</td>\n",
              "      <td>-0.315176</td>\n",
              "      <td>-0.310746</td>\n",
              "      <td>0.443302</td>\n",
              "      <td>0.342528</td>\n",
              "      <td>-0.509311</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.281996</td>\n",
              "      <td>-0.222720</td>\n",
              "      <td>0.289475</td>\n",
              "      <td>0.332439</td>\n",
              "      <td>-0.293604</td>\n",
              "      <td>-0.229673</td>\n",
              "      <td>-0.386515</td>\n",
              "      <td>-0.083441</td>\n",
              "      <td>-0.006928</td>\n",
              "      <td>-0.153248</td>\n",
              "      <td>-0.069010</td>\n",
              "      <td>-0.035604</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.618610</td>\n",
              "      <td>0.143009</td>\n",
              "      <td>0.033791</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.559106</td>\n",
              "      <td>0.343544</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>0.210909</td>\n",
              "      <td>0.052895</td>\n",
              "      <td>0.540812</td>\n",
              "      <td>-1.039226</td>\n",
              "      <td>0.324640</td>\n",
              "      <td>-0.230784</td>\n",
              "      <td>-0.458668</td>\n",
              "      <td>0.864519</td>\n",
              "      <td>-0.330804</td>\n",
              "      <td>0.134833</td>\n",
              "      <td>0.251873</td>\n",
              "      <td>0.115973</td>\n",
              "      <td>0.559405</td>\n",
              "      <td>-0.046733</td>\n",
              "      <td>-0.409944</td>\n",
              "      <td>0.248319</td>\n",
              "      <td>-0.117079</td>\n",
              "      <td>0.029875</td>\n",
              "      <td>0.671931</td>\n",
              "      <td>-0.095137</td>\n",
              "      <td>-0.140844</td>\n",
              "      <td>0.151217</td>\n",
              "      <td>0.199436</td>\n",
              "      <td>0.426605</td>\n",
              "      <td>-0.169045</td>\n",
              "      <td>-0.224876</td>\n",
              "      <td>-0.091870</td>\n",
              "      <td>-0.103131</td>\n",
              "      <td>0.242713</td>\n",
              "      <td>0.245233</td>\n",
              "      <td>0.259683</td>\n",
              "      <td>0.429292</td>\n",
              "      <td>0.173730</td>\n",
              "      <td>0.008138</td>\n",
              "      <td>-0.433647</td>\n",
              "      <td>0.007498</td>\n",
              "      <td>-0.076818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.318171</td>\n",
              "      <td>0.092903</td>\n",
              "      <td>0.258053</td>\n",
              "      <td>0.315763</td>\n",
              "      <td>-0.219837</td>\n",
              "      <td>-0.139032</td>\n",
              "      <td>-0.129200</td>\n",
              "      <td>0.058051</td>\n",
              "      <td>-0.499771</td>\n",
              "      <td>-0.247845</td>\n",
              "      <td>-0.376459</td>\n",
              "      <td>-0.043936</td>\n",
              "      <td>0.144335</td>\n",
              "      <td>0.422860</td>\n",
              "      <td>0.104366</td>\n",
              "      <td>0.659030</td>\n",
              "      <td>-0.370120</td>\n",
              "      <td>0.317236</td>\n",
              "      <td>-0.694991</td>\n",
              "      <td>-0.188938</td>\n",
              "      <td>0.647019</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>-0.584288</td>\n",
              "      <td>0.205208</td>\n",
              "      <td>-0.187555</td>\n",
              "      <td>-0.342430</td>\n",
              "      <td>0.259223</td>\n",
              "      <td>-0.074874</td>\n",
              "      <td>-0.243773</td>\n",
              "      <td>-0.343508</td>\n",
              "      <td>-0.179491</td>\n",
              "      <td>-0.065786</td>\n",
              "      <td>-0.401505</td>\n",
              "      <td>-0.172304</td>\n",
              "      <td>-0.293728</td>\n",
              "      <td>-0.116794</td>\n",
              "      <td>-0.456643</td>\n",
              "      <td>-0.667378</td>\n",
              "      <td>-0.184163</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.357497</td>\n",
              "      <td>0.229890</td>\n",
              "      <td>0.151352</td>\n",
              "      <td>-0.319823</td>\n",
              "      <td>-0.043061</td>\n",
              "      <td>0.312032</td>\n",
              "      <td>-1.122298</td>\n",
              "      <td>0.371870</td>\n",
              "      <td>-0.164523</td>\n",
              "      <td>-0.262540</td>\n",
              "      <td>0.985864</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.545672</td>\n",
              "      <td>0.194945</td>\n",
              "      <td>-0.079880</td>\n",
              "      <td>0.491040</td>\n",
              "      <td>-0.589932</td>\n",
              "      <td>-0.839804</td>\n",
              "      <td>-0.040601</td>\n",
              "      <td>-0.075923</td>\n",
              "      <td>0.076086</td>\n",
              "      <td>0.162593</td>\n",
              "      <td>-0.063084</td>\n",
              "      <td>0.366760</td>\n",
              "      <td>-0.073989</td>\n",
              "      <td>0.197160</td>\n",
              "      <td>0.675615</td>\n",
              "      <td>0.375638</td>\n",
              "      <td>-0.441188</td>\n",
              "      <td>-0.065113</td>\n",
              "      <td>0.010952</td>\n",
              "      <td>0.134076</td>\n",
              "      <td>0.250615</td>\n",
              "      <td>0.694893</td>\n",
              "      <td>0.260527</td>\n",
              "      <td>0.527698</td>\n",
              "      <td>0.081689</td>\n",
              "      <td>-0.534914</td>\n",
              "      <td>-0.187401</td>\n",
              "      <td>-0.595615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "0  0.351841  0.350937  0.328896  ... -0.834121 -0.221069 -0.150381\n",
              "1  0.690412 -0.050015  0.213738  ... -0.711145  0.001425 -0.138732\n",
              "2  0.878922  0.129748  0.209894  ... -0.783624 -0.406802 -0.646021\n",
              "3  0.131087  0.040849  0.268820  ... -0.433647  0.007498 -0.076818\n",
              "4  0.318171  0.092903  0.258053  ... -0.534914 -0.187401 -0.595615\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "0Yl4N89TYT22",
        "outputId": "01ec6f30-bb42-4e68-c999-5004a4a3d227"
      },
      "source": [
        "# The second one will contain both the embedding of the responses and of the messages they were directed to.\n",
        "X_embedding_2_forum_dataset=pd.DataFrame(original_and_response_embedding)\n",
        "\n",
        "X_embedding_2_forum_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351841</td>\n",
              "      <td>0.350937</td>\n",
              "      <td>0.328896</td>\n",
              "      <td>0.188909</td>\n",
              "      <td>-0.705771</td>\n",
              "      <td>-0.108164</td>\n",
              "      <td>0.169885</td>\n",
              "      <td>0.651574</td>\n",
              "      <td>-0.232430</td>\n",
              "      <td>-0.057164</td>\n",
              "      <td>-0.312394</td>\n",
              "      <td>-0.081075</td>\n",
              "      <td>0.933955</td>\n",
              "      <td>0.850431</td>\n",
              "      <td>0.723808</td>\n",
              "      <td>0.961155</td>\n",
              "      <td>-0.370502</td>\n",
              "      <td>0.407380</td>\n",
              "      <td>-0.213364</td>\n",
              "      <td>-0.257225</td>\n",
              "      <td>1.045631</td>\n",
              "      <td>0.036601</td>\n",
              "      <td>-1.329234</td>\n",
              "      <td>0.147800</td>\n",
              "      <td>0.043450</td>\n",
              "      <td>-0.213277</td>\n",
              "      <td>0.171580</td>\n",
              "      <td>-0.036937</td>\n",
              "      <td>-0.014714</td>\n",
              "      <td>-0.256418</td>\n",
              "      <td>-0.444057</td>\n",
              "      <td>-0.288581</td>\n",
              "      <td>-0.016435</td>\n",
              "      <td>-0.016690</td>\n",
              "      <td>-0.651222</td>\n",
              "      <td>-0.105144</td>\n",
              "      <td>-0.123457</td>\n",
              "      <td>-0.719637</td>\n",
              "      <td>0.135821</td>\n",
              "      <td>-0.371884</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.180577</td>\n",
              "      <td>-0.076559</td>\n",
              "      <td>0.329436</td>\n",
              "      <td>0.200916</td>\n",
              "      <td>-0.668115</td>\n",
              "      <td>0.326379</td>\n",
              "      <td>0.059194</td>\n",
              "      <td>0.236648</td>\n",
              "      <td>0.011461</td>\n",
              "      <td>0.085470</td>\n",
              "      <td>-0.035423</td>\n",
              "      <td>0.613259</td>\n",
              "      <td>-0.097791</td>\n",
              "      <td>0.194686</td>\n",
              "      <td>0.385597</td>\n",
              "      <td>-0.396243</td>\n",
              "      <td>-0.155190</td>\n",
              "      <td>0.113475</td>\n",
              "      <td>1.303838</td>\n",
              "      <td>-0.013605</td>\n",
              "      <td>-0.345676</td>\n",
              "      <td>-0.511204</td>\n",
              "      <td>-0.143413</td>\n",
              "      <td>0.124918</td>\n",
              "      <td>0.036609</td>\n",
              "      <td>-0.347161</td>\n",
              "      <td>-0.113807</td>\n",
              "      <td>0.444176</td>\n",
              "      <td>-0.055736</td>\n",
              "      <td>0.318379</td>\n",
              "      <td>0.441543</td>\n",
              "      <td>-0.378829</td>\n",
              "      <td>-0.369618</td>\n",
              "      <td>-0.009468</td>\n",
              "      <td>-0.027412</td>\n",
              "      <td>0.375281</td>\n",
              "      <td>-0.243132</td>\n",
              "      <td>0.190220</td>\n",
              "      <td>-0.183785</td>\n",
              "      <td>-0.099087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.690412</td>\n",
              "      <td>-0.050015</td>\n",
              "      <td>0.213738</td>\n",
              "      <td>0.477493</td>\n",
              "      <td>-0.355049</td>\n",
              "      <td>-0.184899</td>\n",
              "      <td>0.370410</td>\n",
              "      <td>0.576586</td>\n",
              "      <td>-0.028780</td>\n",
              "      <td>-0.266123</td>\n",
              "      <td>-0.594824</td>\n",
              "      <td>-0.065822</td>\n",
              "      <td>0.635375</td>\n",
              "      <td>0.495326</td>\n",
              "      <td>0.178782</td>\n",
              "      <td>0.609307</td>\n",
              "      <td>0.173820</td>\n",
              "      <td>0.369583</td>\n",
              "      <td>-0.523248</td>\n",
              "      <td>-0.345976</td>\n",
              "      <td>0.784168</td>\n",
              "      <td>0.135446</td>\n",
              "      <td>-0.725013</td>\n",
              "      <td>0.278725</td>\n",
              "      <td>-0.308128</td>\n",
              "      <td>-0.372479</td>\n",
              "      <td>0.841734</td>\n",
              "      <td>-0.183544</td>\n",
              "      <td>0.254608</td>\n",
              "      <td>0.161915</td>\n",
              "      <td>-0.415911</td>\n",
              "      <td>0.008910</td>\n",
              "      <td>-0.327533</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>-0.478026</td>\n",
              "      <td>-0.349667</td>\n",
              "      <td>-0.299333</td>\n",
              "      <td>-0.614111</td>\n",
              "      <td>-0.161426</td>\n",
              "      <td>-0.290567</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.105561</td>\n",
              "      <td>0.513157</td>\n",
              "      <td>0.461567</td>\n",
              "      <td>-0.001638</td>\n",
              "      <td>-0.499008</td>\n",
              "      <td>0.019787</td>\n",
              "      <td>0.309282</td>\n",
              "      <td>-0.025571</td>\n",
              "      <td>-0.216489</td>\n",
              "      <td>-0.055935</td>\n",
              "      <td>0.040311</td>\n",
              "      <td>0.424981</td>\n",
              "      <td>-0.409014</td>\n",
              "      <td>0.338961</td>\n",
              "      <td>0.276281</td>\n",
              "      <td>0.256482</td>\n",
              "      <td>0.170107</td>\n",
              "      <td>-0.081534</td>\n",
              "      <td>0.289291</td>\n",
              "      <td>0.132700</td>\n",
              "      <td>-0.021646</td>\n",
              "      <td>-0.013135</td>\n",
              "      <td>-0.373703</td>\n",
              "      <td>0.052994</td>\n",
              "      <td>0.040370</td>\n",
              "      <td>0.137881</td>\n",
              "      <td>-0.289646</td>\n",
              "      <td>0.413540</td>\n",
              "      <td>0.375469</td>\n",
              "      <td>0.214657</td>\n",
              "      <td>0.225897</td>\n",
              "      <td>0.152689</td>\n",
              "      <td>-0.335577</td>\n",
              "      <td>0.086263</td>\n",
              "      <td>0.034447</td>\n",
              "      <td>0.251328</td>\n",
              "      <td>0.207100</td>\n",
              "      <td>0.067244</td>\n",
              "      <td>-0.406278</td>\n",
              "      <td>-0.110736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.878922</td>\n",
              "      <td>0.129748</td>\n",
              "      <td>0.209894</td>\n",
              "      <td>0.477275</td>\n",
              "      <td>-0.013467</td>\n",
              "      <td>-0.034873</td>\n",
              "      <td>0.166108</td>\n",
              "      <td>0.566422</td>\n",
              "      <td>-0.104887</td>\n",
              "      <td>-0.361374</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>-0.308725</td>\n",
              "      <td>0.359111</td>\n",
              "      <td>0.733559</td>\n",
              "      <td>0.259489</td>\n",
              "      <td>0.489453</td>\n",
              "      <td>-0.126814</td>\n",
              "      <td>0.459890</td>\n",
              "      <td>-0.236492</td>\n",
              "      <td>-0.534941</td>\n",
              "      <td>0.799984</td>\n",
              "      <td>0.049314</td>\n",
              "      <td>-0.792654</td>\n",
              "      <td>0.095532</td>\n",
              "      <td>-0.057992</td>\n",
              "      <td>-0.682315</td>\n",
              "      <td>0.595183</td>\n",
              "      <td>0.225203</td>\n",
              "      <td>0.173171</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.148691</td>\n",
              "      <td>0.280422</td>\n",
              "      <td>-0.013118</td>\n",
              "      <td>-0.173365</td>\n",
              "      <td>-0.611526</td>\n",
              "      <td>-0.524104</td>\n",
              "      <td>-0.158826</td>\n",
              "      <td>-0.621647</td>\n",
              "      <td>-0.106134</td>\n",
              "      <td>0.120485</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.528560</td>\n",
              "      <td>-0.050407</td>\n",
              "      <td>0.501908</td>\n",
              "      <td>0.414291</td>\n",
              "      <td>-0.185858</td>\n",
              "      <td>0.283569</td>\n",
              "      <td>0.763650</td>\n",
              "      <td>0.237737</td>\n",
              "      <td>-0.049269</td>\n",
              "      <td>0.082608</td>\n",
              "      <td>-0.242887</td>\n",
              "      <td>0.192132</td>\n",
              "      <td>-0.101484</td>\n",
              "      <td>0.341497</td>\n",
              "      <td>-0.032177</td>\n",
              "      <td>-0.369832</td>\n",
              "      <td>0.020080</td>\n",
              "      <td>0.074823</td>\n",
              "      <td>0.526132</td>\n",
              "      <td>-0.218488</td>\n",
              "      <td>-0.204755</td>\n",
              "      <td>-0.203179</td>\n",
              "      <td>-0.597357</td>\n",
              "      <td>0.106558</td>\n",
              "      <td>0.495821</td>\n",
              "      <td>-0.285147</td>\n",
              "      <td>0.053054</td>\n",
              "      <td>-0.044256</td>\n",
              "      <td>0.061676</td>\n",
              "      <td>0.074897</td>\n",
              "      <td>0.650662</td>\n",
              "      <td>0.172320</td>\n",
              "      <td>0.138465</td>\n",
              "      <td>-0.031338</td>\n",
              "      <td>-0.177022</td>\n",
              "      <td>0.034096</td>\n",
              "      <td>-0.166424</td>\n",
              "      <td>0.139723</td>\n",
              "      <td>0.001948</td>\n",
              "      <td>0.396553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.131087</td>\n",
              "      <td>0.040849</td>\n",
              "      <td>0.268820</td>\n",
              "      <td>0.166719</td>\n",
              "      <td>-0.283283</td>\n",
              "      <td>-0.121891</td>\n",
              "      <td>0.127263</td>\n",
              "      <td>0.330898</td>\n",
              "      <td>-0.481086</td>\n",
              "      <td>0.066487</td>\n",
              "      <td>0.104336</td>\n",
              "      <td>-0.175074</td>\n",
              "      <td>0.413281</td>\n",
              "      <td>0.214315</td>\n",
              "      <td>-0.010617</td>\n",
              "      <td>0.453170</td>\n",
              "      <td>-0.272459</td>\n",
              "      <td>-0.008220</td>\n",
              "      <td>-0.315176</td>\n",
              "      <td>-0.310746</td>\n",
              "      <td>0.443302</td>\n",
              "      <td>0.342528</td>\n",
              "      <td>-0.509311</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.281996</td>\n",
              "      <td>-0.222720</td>\n",
              "      <td>0.289475</td>\n",
              "      <td>0.332439</td>\n",
              "      <td>-0.293604</td>\n",
              "      <td>-0.229673</td>\n",
              "      <td>-0.386515</td>\n",
              "      <td>-0.083441</td>\n",
              "      <td>-0.006928</td>\n",
              "      <td>-0.153248</td>\n",
              "      <td>-0.069010</td>\n",
              "      <td>-0.035604</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.618610</td>\n",
              "      <td>0.143009</td>\n",
              "      <td>0.033791</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017763</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>0.024758</td>\n",
              "      <td>-0.217229</td>\n",
              "      <td>0.089635</td>\n",
              "      <td>-0.446333</td>\n",
              "      <td>-0.221698</td>\n",
              "      <td>-0.200670</td>\n",
              "      <td>0.139280</td>\n",
              "      <td>0.143303</td>\n",
              "      <td>0.060085</td>\n",
              "      <td>0.094824</td>\n",
              "      <td>0.030719</td>\n",
              "      <td>-0.140576</td>\n",
              "      <td>-0.118547</td>\n",
              "      <td>-0.248959</td>\n",
              "      <td>-0.100608</td>\n",
              "      <td>-0.126300</td>\n",
              "      <td>-0.132032</td>\n",
              "      <td>0.040979</td>\n",
              "      <td>0.203699</td>\n",
              "      <td>-0.312565</td>\n",
              "      <td>0.154701</td>\n",
              "      <td>0.453629</td>\n",
              "      <td>0.117124</td>\n",
              "      <td>-0.032653</td>\n",
              "      <td>0.031953</td>\n",
              "      <td>0.187257</td>\n",
              "      <td>0.047782</td>\n",
              "      <td>0.017481</td>\n",
              "      <td>-0.000990</td>\n",
              "      <td>0.120420</td>\n",
              "      <td>-0.290606</td>\n",
              "      <td>0.382589</td>\n",
              "      <td>-0.098086</td>\n",
              "      <td>0.155773</td>\n",
              "      <td>-0.055415</td>\n",
              "      <td>0.033166</td>\n",
              "      <td>-0.244312</td>\n",
              "      <td>-0.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.318171</td>\n",
              "      <td>0.092903</td>\n",
              "      <td>0.258053</td>\n",
              "      <td>0.315763</td>\n",
              "      <td>-0.219837</td>\n",
              "      <td>-0.139032</td>\n",
              "      <td>-0.129200</td>\n",
              "      <td>0.058051</td>\n",
              "      <td>-0.499771</td>\n",
              "      <td>-0.247845</td>\n",
              "      <td>-0.376459</td>\n",
              "      <td>-0.043936</td>\n",
              "      <td>0.144335</td>\n",
              "      <td>0.422860</td>\n",
              "      <td>0.104366</td>\n",
              "      <td>0.659030</td>\n",
              "      <td>-0.370120</td>\n",
              "      <td>0.317236</td>\n",
              "      <td>-0.694991</td>\n",
              "      <td>-0.188938</td>\n",
              "      <td>0.647019</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>-0.584288</td>\n",
              "      <td>0.205208</td>\n",
              "      <td>-0.187555</td>\n",
              "      <td>-0.342430</td>\n",
              "      <td>0.259223</td>\n",
              "      <td>-0.074874</td>\n",
              "      <td>-0.243773</td>\n",
              "      <td>-0.343508</td>\n",
              "      <td>-0.179491</td>\n",
              "      <td>-0.065786</td>\n",
              "      <td>-0.401505</td>\n",
              "      <td>-0.172304</td>\n",
              "      <td>-0.293728</td>\n",
              "      <td>-0.116794</td>\n",
              "      <td>-0.456643</td>\n",
              "      <td>-0.667378</td>\n",
              "      <td>-0.184163</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.201610</td>\n",
              "      <td>0.113654</td>\n",
              "      <td>-0.113997</td>\n",
              "      <td>0.530732</td>\n",
              "      <td>0.095956</td>\n",
              "      <td>0.228781</td>\n",
              "      <td>0.083072</td>\n",
              "      <td>-0.047230</td>\n",
              "      <td>-0.066261</td>\n",
              "      <td>-0.196128</td>\n",
              "      <td>-0.121345</td>\n",
              "      <td>-0.292609</td>\n",
              "      <td>-0.410839</td>\n",
              "      <td>0.056928</td>\n",
              "      <td>0.195852</td>\n",
              "      <td>0.068365</td>\n",
              "      <td>0.543199</td>\n",
              "      <td>0.429861</td>\n",
              "      <td>0.288920</td>\n",
              "      <td>-0.041156</td>\n",
              "      <td>-0.046212</td>\n",
              "      <td>0.509338</td>\n",
              "      <td>-0.032053</td>\n",
              "      <td>-0.507604</td>\n",
              "      <td>0.225206</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>-0.249010</td>\n",
              "      <td>-0.544683</td>\n",
              "      <td>0.216313</td>\n",
              "      <td>-0.026757</td>\n",
              "      <td>-0.114083</td>\n",
              "      <td>0.108637</td>\n",
              "      <td>-0.005382</td>\n",
              "      <td>-0.435209</td>\n",
              "      <td>0.168765</td>\n",
              "      <td>-0.353968</td>\n",
              "      <td>-0.073551</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>0.194899</td>\n",
              "      <td>0.518797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       197       198       199\n",
              "0  0.351841  0.350937  0.328896  ...  0.190220 -0.183785 -0.099087\n",
              "1  0.690412 -0.050015  0.213738  ...  0.067244 -0.406278 -0.110736\n",
              "2  0.878922  0.129748  0.209894  ...  0.139723  0.001948  0.396553\n",
              "3  0.131087  0.040849  0.268820  ...  0.033166 -0.244312 -0.261200\n",
              "4  0.318171  0.092903  0.258053  ...  0.101267  0.194899  0.518797\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "C_muR9SidSSl",
        "outputId": "f94022a0-ba11-43b1-e44d-c311aa12e841"
      },
      "source": [
        "# The third one will contain the embedding of the responses and of the messages they were directed to, and the bag of words rappresentation\n",
        "# of the responses messages.\n",
        "\n",
        "X_combined_forum_dataset=pd.DataFrame(combined_list)\n",
        "\n",
        "X_combined_forum_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>8425</th>\n",
              "      <th>8426</th>\n",
              "      <th>8427</th>\n",
              "      <th>8428</th>\n",
              "      <th>8429</th>\n",
              "      <th>8430</th>\n",
              "      <th>8431</th>\n",
              "      <th>8432</th>\n",
              "      <th>8433</th>\n",
              "      <th>8434</th>\n",
              "      <th>8435</th>\n",
              "      <th>8436</th>\n",
              "      <th>8437</th>\n",
              "      <th>8438</th>\n",
              "      <th>8439</th>\n",
              "      <th>8440</th>\n",
              "      <th>8441</th>\n",
              "      <th>8442</th>\n",
              "      <th>8443</th>\n",
              "      <th>8444</th>\n",
              "      <th>8445</th>\n",
              "      <th>8446</th>\n",
              "      <th>8447</th>\n",
              "      <th>8448</th>\n",
              "      <th>8449</th>\n",
              "      <th>8450</th>\n",
              "      <th>8451</th>\n",
              "      <th>8452</th>\n",
              "      <th>8453</th>\n",
              "      <th>8454</th>\n",
              "      <th>8455</th>\n",
              "      <th>8456</th>\n",
              "      <th>8457</th>\n",
              "      <th>8458</th>\n",
              "      <th>8459</th>\n",
              "      <th>8460</th>\n",
              "      <th>8461</th>\n",
              "      <th>8462</th>\n",
              "      <th>8463</th>\n",
              "      <th>8464</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.351841</td>\n",
              "      <td>0.350937</td>\n",
              "      <td>0.328896</td>\n",
              "      <td>0.188909</td>\n",
              "      <td>-0.705771</td>\n",
              "      <td>-0.108164</td>\n",
              "      <td>0.169885</td>\n",
              "      <td>0.651574</td>\n",
              "      <td>-0.232430</td>\n",
              "      <td>-0.057164</td>\n",
              "      <td>-0.312394</td>\n",
              "      <td>-0.081075</td>\n",
              "      <td>0.933955</td>\n",
              "      <td>0.850431</td>\n",
              "      <td>0.723808</td>\n",
              "      <td>0.961155</td>\n",
              "      <td>-0.370502</td>\n",
              "      <td>0.407380</td>\n",
              "      <td>-0.213364</td>\n",
              "      <td>-0.257225</td>\n",
              "      <td>1.045631</td>\n",
              "      <td>0.036601</td>\n",
              "      <td>-1.329234</td>\n",
              "      <td>0.147800</td>\n",
              "      <td>0.043450</td>\n",
              "      <td>-0.213277</td>\n",
              "      <td>0.171580</td>\n",
              "      <td>-0.036937</td>\n",
              "      <td>-0.014714</td>\n",
              "      <td>-0.256418</td>\n",
              "      <td>-0.444057</td>\n",
              "      <td>-0.288581</td>\n",
              "      <td>-0.016435</td>\n",
              "      <td>-0.016690</td>\n",
              "      <td>-0.651222</td>\n",
              "      <td>-0.105144</td>\n",
              "      <td>-0.123457</td>\n",
              "      <td>-0.719637</td>\n",
              "      <td>0.135821</td>\n",
              "      <td>-0.371884</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135610</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.690412</td>\n",
              "      <td>-0.050015</td>\n",
              "      <td>0.213738</td>\n",
              "      <td>0.477493</td>\n",
              "      <td>-0.355049</td>\n",
              "      <td>-0.184899</td>\n",
              "      <td>0.370410</td>\n",
              "      <td>0.576586</td>\n",
              "      <td>-0.028780</td>\n",
              "      <td>-0.266123</td>\n",
              "      <td>-0.594824</td>\n",
              "      <td>-0.065822</td>\n",
              "      <td>0.635375</td>\n",
              "      <td>0.495326</td>\n",
              "      <td>0.178782</td>\n",
              "      <td>0.609307</td>\n",
              "      <td>0.173820</td>\n",
              "      <td>0.369583</td>\n",
              "      <td>-0.523248</td>\n",
              "      <td>-0.345976</td>\n",
              "      <td>0.784168</td>\n",
              "      <td>0.135446</td>\n",
              "      <td>-0.725013</td>\n",
              "      <td>0.278725</td>\n",
              "      <td>-0.308128</td>\n",
              "      <td>-0.372479</td>\n",
              "      <td>0.841734</td>\n",
              "      <td>-0.183544</td>\n",
              "      <td>0.254608</td>\n",
              "      <td>0.161915</td>\n",
              "      <td>-0.415911</td>\n",
              "      <td>0.008910</td>\n",
              "      <td>-0.327533</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>-0.478026</td>\n",
              "      <td>-0.349667</td>\n",
              "      <td>-0.299333</td>\n",
              "      <td>-0.614111</td>\n",
              "      <td>-0.161426</td>\n",
              "      <td>-0.290567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.878922</td>\n",
              "      <td>0.129748</td>\n",
              "      <td>0.209894</td>\n",
              "      <td>0.477275</td>\n",
              "      <td>-0.013467</td>\n",
              "      <td>-0.034873</td>\n",
              "      <td>0.166108</td>\n",
              "      <td>0.566422</td>\n",
              "      <td>-0.104887</td>\n",
              "      <td>-0.361374</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>-0.308725</td>\n",
              "      <td>0.359111</td>\n",
              "      <td>0.733559</td>\n",
              "      <td>0.259489</td>\n",
              "      <td>0.489453</td>\n",
              "      <td>-0.126814</td>\n",
              "      <td>0.459890</td>\n",
              "      <td>-0.236492</td>\n",
              "      <td>-0.534941</td>\n",
              "      <td>0.799984</td>\n",
              "      <td>0.049314</td>\n",
              "      <td>-0.792654</td>\n",
              "      <td>0.095532</td>\n",
              "      <td>-0.057992</td>\n",
              "      <td>-0.682315</td>\n",
              "      <td>0.595183</td>\n",
              "      <td>0.225203</td>\n",
              "      <td>0.173171</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.148691</td>\n",
              "      <td>0.280422</td>\n",
              "      <td>-0.013118</td>\n",
              "      <td>-0.173365</td>\n",
              "      <td>-0.611526</td>\n",
              "      <td>-0.524104</td>\n",
              "      <td>-0.158826</td>\n",
              "      <td>-0.621647</td>\n",
              "      <td>-0.106134</td>\n",
              "      <td>0.120485</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.131087</td>\n",
              "      <td>0.040849</td>\n",
              "      <td>0.268820</td>\n",
              "      <td>0.166719</td>\n",
              "      <td>-0.283283</td>\n",
              "      <td>-0.121891</td>\n",
              "      <td>0.127263</td>\n",
              "      <td>0.330898</td>\n",
              "      <td>-0.481086</td>\n",
              "      <td>0.066487</td>\n",
              "      <td>0.104336</td>\n",
              "      <td>-0.175074</td>\n",
              "      <td>0.413281</td>\n",
              "      <td>0.214315</td>\n",
              "      <td>-0.010617</td>\n",
              "      <td>0.453170</td>\n",
              "      <td>-0.272459</td>\n",
              "      <td>-0.008220</td>\n",
              "      <td>-0.315176</td>\n",
              "      <td>-0.310746</td>\n",
              "      <td>0.443302</td>\n",
              "      <td>0.342528</td>\n",
              "      <td>-0.509311</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.281996</td>\n",
              "      <td>-0.222720</td>\n",
              "      <td>0.289475</td>\n",
              "      <td>0.332439</td>\n",
              "      <td>-0.293604</td>\n",
              "      <td>-0.229673</td>\n",
              "      <td>-0.386515</td>\n",
              "      <td>-0.083441</td>\n",
              "      <td>-0.006928</td>\n",
              "      <td>-0.153248</td>\n",
              "      <td>-0.069010</td>\n",
              "      <td>-0.035604</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.618610</td>\n",
              "      <td>0.143009</td>\n",
              "      <td>0.033791</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.082292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.318171</td>\n",
              "      <td>0.092903</td>\n",
              "      <td>0.258053</td>\n",
              "      <td>0.315763</td>\n",
              "      <td>-0.219837</td>\n",
              "      <td>-0.139032</td>\n",
              "      <td>-0.129200</td>\n",
              "      <td>0.058051</td>\n",
              "      <td>-0.499771</td>\n",
              "      <td>-0.247845</td>\n",
              "      <td>-0.376459</td>\n",
              "      <td>-0.043936</td>\n",
              "      <td>0.144335</td>\n",
              "      <td>0.422860</td>\n",
              "      <td>0.104366</td>\n",
              "      <td>0.659030</td>\n",
              "      <td>-0.370120</td>\n",
              "      <td>0.317236</td>\n",
              "      <td>-0.694991</td>\n",
              "      <td>-0.188938</td>\n",
              "      <td>0.647019</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>-0.584288</td>\n",
              "      <td>0.205208</td>\n",
              "      <td>-0.187555</td>\n",
              "      <td>-0.342430</td>\n",
              "      <td>0.259223</td>\n",
              "      <td>-0.074874</td>\n",
              "      <td>-0.243773</td>\n",
              "      <td>-0.343508</td>\n",
              "      <td>-0.179491</td>\n",
              "      <td>-0.065786</td>\n",
              "      <td>-0.401505</td>\n",
              "      <td>-0.172304</td>\n",
              "      <td>-0.293728</td>\n",
              "      <td>-0.116794</td>\n",
              "      <td>-0.456643</td>\n",
              "      <td>-0.667378</td>\n",
              "      <td>-0.184163</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  8465 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3     ...  8461  8462  8463  8464\n",
              "0  0.351841  0.350937  0.328896  0.188909  ...   0.0   0.0   0.0   0.0\n",
              "1  0.690412 -0.050015  0.213738  0.477493  ...   0.0   0.0   0.0   0.0\n",
              "2  0.878922  0.129748  0.209894  0.477275  ...   0.0   0.0   0.0   0.0\n",
              "3  0.131087  0.040849  0.268820  0.166719  ...   0.0   0.0   0.0   0.0\n",
              "4  0.318171  0.092903  0.258053  0.315763  ...   0.0   0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 8465 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PndLYZlMPa0p"
      },
      "source": [
        "**Models comparison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-VQoVacPY4e"
      },
      "source": [
        "# Dictionary that associates a name to each of the 4 dataset types.\n",
        "Datasets={'vector':X_vector_forum_dataset,'embedding_response':X_embedding_1_forum_dataset,'embedding_original':X_embedding_2_forum_dataset,'combined':X_combined_forum_dataset}\n",
        "\n",
        "# Lists containing the names of the datasets and the models for supervised and unsupervised learning.\n",
        "dataset_names=['vector','embedding_response','embedding_original','combined']\n",
        "unsupervised_names=['kmeans','hierarchical']\n",
        "supervised_names=['knn','tree','svm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIFoEI1MdwQm"
      },
      "source": [
        "**Unsupervised learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eihMP3CNjHh"
      },
      "source": [
        "def def_unsupervised_models():\n",
        "  models={\n",
        "    'kmeans':KMeans(n_clusters=7, random_state=0),\n",
        "    'hierarchical':AgglomerativeClustering(n_clusters=7)\n",
        "  }\n",
        "\n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRa3Y1qaQcFl",
        "outputId": "9d273a7e-a16b-497d-8636-b6236f0a7490"
      },
      "source": [
        "# Create a list with 4 elements, one for each dataset type, where each element contais the predicted labels for\n",
        "# both the 2 unsupervised models chosed.\n",
        "unsupervised_models={}\n",
        "\n",
        "for dataset in dataset_names:\n",
        "  unsupervised_models[dataset]=def_unsupervised_models()\n",
        "\n",
        "# Fit the models and predict the cluster of the instances for each one of the 4 dataset types.\n",
        "for dataset in dataset_names:\n",
        "  models=unsupervised_models[dataset]\n",
        "  for name,model in models.items():\n",
        "    print(\"Training: {}, dataset {}\".format(name,dataset))\n",
        "    X_train=Datasets[dataset]\n",
        "    models[name]=model.fit_predict(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: kmeans, dataset vector\n",
            "Training: hierarchical, dataset vector\n",
            "Training: kmeans, dataset embedding_response\n",
            "Training: hierarchical, dataset embedding_response\n",
            "Training: kmeans, dataset embedding_original\n",
            "Training: hierarchical, dataset embedding_original\n",
            "Training: kmeans, dataset combined\n",
            "Training: hierarchical, dataset combined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOhp_XQjXXxp"
      },
      "source": [
        "# Memorize the rand scores in a dictionary named rand_scores.\n",
        "rand_scores={}\n",
        "\n",
        "# The dictionary will have an entry for each dataset type.\n",
        "for dataset in dataset_names:\n",
        "  rand_scores[dataset]={}\n",
        "\n",
        "# Fit the models for each one of the 4 dataset types.\n",
        "for dataset in dataset_names:\n",
        "  models=unsupervised_models[dataset]\n",
        "  scores=rand_scores[dataset]\n",
        "  for name,model in models.items():\n",
        "    scores[name]=adjusted_rand_score(y_forum,models[name])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKQ3A9C9vE2I",
        "outputId": "bca9d48e-c7ce-46ea-e2bd-aadf3a486da2"
      },
      "source": [
        "print('{:<25} {:<25} {:<15}'.format('dataset' ,'model', 'rand score'))\n",
        "\n",
        "for d_names,dataset_scores in rand_scores.items():\n",
        "  for name, value in dataset_scores.items():\n",
        "    print('{:<25} {:<25} {:<15}'.format(d_names ,name, round(value,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset                   model                     rand score     \n",
            "vector                    kmeans                    0.02           \n",
            "vector                    hierarchical              -0.02          \n",
            "embedding_response        kmeans                    0.03           \n",
            "embedding_response        hierarchical              0.05           \n",
            "embedding_original        kmeans                    0.03           \n",
            "embedding_original        hierarchical              0.04           \n",
            "combined                  kmeans                    0.03           \n",
            "combined                  hierarchical              0.03           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoP2dAjujQK2"
      },
      "source": [
        "**Supervised learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_FllFrJXw0W"
      },
      "source": [
        "def def_supervised_models():\n",
        "  models={\n",
        "    'knn':KNeighborsClassifier(n_neighbors=10),\n",
        "    'tree':DecisionTreeClassifier(random_state=0),\n",
        "    'svm':SVC()\n",
        "  }\n",
        "\n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IPasM11X117",
        "outputId": "9124882e-f383-46f9-b2ae-983bb245f425"
      },
      "source": [
        "# Do training dividing the dataset throgh 5 folds.\n",
        "\n",
        "# Create a dictinary with 4 elements, one for each dataset type.\n",
        "# Each element is a dictinary which contais 5 elements, one for each possible partition of the dataset.\n",
        "# For each of this 5 possible partitions, 3 supervised models chosed are trained.\n",
        "supervised_models={}\n",
        "\n",
        "kf=KFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "for dataset in dataset_names:\n",
        "  supervised_models[dataset]={}\n",
        "  for iteration in range(5):\n",
        "    supervised_models[dataset][iteration]=def_supervised_models()\n",
        "\n",
        "# Fit the models for each one of the 4 dataset types.\n",
        "for dataset in dataset_names:\n",
        "  dataset_iteration=Datasets[dataset]\n",
        "  iteration_k_fold=0\n",
        "\n",
        "  # Apply K-fold with 5 folds.\n",
        "  for train_set,_ in kf.split(dataset_iteration,y_forum):\n",
        "    models=supervised_models[dataset][iteration_k_fold]\n",
        "    iteration_k_fold +=1\n",
        "\n",
        "    # Select dataset rows for the iteration.\n",
        "    X_train=dataset_iteration.loc[train_set]\n",
        "    # Select labels for the iteration.\n",
        "    y_train=[y_forum[j] for j in train_set]\n",
        "\n",
        "    for name,model in models.items():\n",
        "      print(\"Training: {}, dataset {}, iteration {}\".format(name,dataset,str(iteration_k_fold)))\n",
        "      models[name]=model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: knn, dataset vector, iteration 1\n",
            "Training: tree, dataset vector, iteration 1\n",
            "Training: svm, dataset vector, iteration 1\n",
            "Training: knn, dataset vector, iteration 2\n",
            "Training: tree, dataset vector, iteration 2\n",
            "Training: svm, dataset vector, iteration 2\n",
            "Training: knn, dataset vector, iteration 3\n",
            "Training: tree, dataset vector, iteration 3\n",
            "Training: svm, dataset vector, iteration 3\n",
            "Training: knn, dataset vector, iteration 4\n",
            "Training: tree, dataset vector, iteration 4\n",
            "Training: svm, dataset vector, iteration 4\n",
            "Training: knn, dataset vector, iteration 5\n",
            "Training: tree, dataset vector, iteration 5\n",
            "Training: svm, dataset vector, iteration 5\n",
            "Training: knn, dataset embedding_response, iteration 1\n",
            "Training: tree, dataset embedding_response, iteration 1\n",
            "Training: svm, dataset embedding_response, iteration 1\n",
            "Training: knn, dataset embedding_response, iteration 2\n",
            "Training: tree, dataset embedding_response, iteration 2\n",
            "Training: svm, dataset embedding_response, iteration 2\n",
            "Training: knn, dataset embedding_response, iteration 3\n",
            "Training: tree, dataset embedding_response, iteration 3\n",
            "Training: svm, dataset embedding_response, iteration 3\n",
            "Training: knn, dataset embedding_response, iteration 4\n",
            "Training: tree, dataset embedding_response, iteration 4\n",
            "Training: svm, dataset embedding_response, iteration 4\n",
            "Training: knn, dataset embedding_response, iteration 5\n",
            "Training: tree, dataset embedding_response, iteration 5\n",
            "Training: svm, dataset embedding_response, iteration 5\n",
            "Training: knn, dataset embedding_original, iteration 1\n",
            "Training: tree, dataset embedding_original, iteration 1\n",
            "Training: svm, dataset embedding_original, iteration 1\n",
            "Training: knn, dataset embedding_original, iteration 2\n",
            "Training: tree, dataset embedding_original, iteration 2\n",
            "Training: svm, dataset embedding_original, iteration 2\n",
            "Training: knn, dataset embedding_original, iteration 3\n",
            "Training: tree, dataset embedding_original, iteration 3\n",
            "Training: svm, dataset embedding_original, iteration 3\n",
            "Training: knn, dataset embedding_original, iteration 4\n",
            "Training: tree, dataset embedding_original, iteration 4\n",
            "Training: svm, dataset embedding_original, iteration 4\n",
            "Training: knn, dataset embedding_original, iteration 5\n",
            "Training: tree, dataset embedding_original, iteration 5\n",
            "Training: svm, dataset embedding_original, iteration 5\n",
            "Training: knn, dataset combined, iteration 1\n",
            "Training: tree, dataset combined, iteration 1\n",
            "Training: svm, dataset combined, iteration 1\n",
            "Training: knn, dataset combined, iteration 2\n",
            "Training: tree, dataset combined, iteration 2\n",
            "Training: svm, dataset combined, iteration 2\n",
            "Training: knn, dataset combined, iteration 3\n",
            "Training: tree, dataset combined, iteration 3\n",
            "Training: svm, dataset combined, iteration 3\n",
            "Training: knn, dataset combined, iteration 4\n",
            "Training: tree, dataset combined, iteration 4\n",
            "Training: svm, dataset combined, iteration 4\n",
            "Training: knn, dataset combined, iteration 5\n",
            "Training: tree, dataset combined, iteration 5\n",
            "Training: svm, dataset combined, iteration 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAeWFB5PW76y",
        "outputId": "370aead9-51ae-4a1c-a945-ab7318913e24"
      },
      "source": [
        "# Memorize the precision, recall and f1 scores in three dictionaries.\n",
        "precision_scores={}\n",
        "recall_scores={}\n",
        "f1_scores={}\n",
        "\n",
        "# The dictionary will have an entry for each dataset type.\n",
        "for dataset in dataset_names:\n",
        "  precision_scores[dataset]={}\n",
        "  recall_scores[dataset]={}\n",
        "  f1_scores[dataset]={}\n",
        "  for model in supervised_names:\n",
        "    precision_scores[dataset][model]=0.\n",
        "    recall_scores[dataset][model]=0.\n",
        "    f1_scores[dataset][model]=0.\n",
        "\n",
        "# Find the predictions for each model of each one of the 4 dataset types.\n",
        "for dataset in dataset_names:\n",
        "  dataset_iteration=Datasets[dataset]\n",
        "  scores_p=precision_scores[dataset]\n",
        "  scores_r=recall_scores[dataset]\n",
        "  scores_f=f1_scores[dataset]\n",
        "  iteration_k_fold=0\n",
        "\n",
        "  # Apply K-fold with 5 folds.\n",
        "  for _,test_set in kf.split(dataset_iteration,y_forum):\n",
        "    models=supervised_models[dataset][iteration_k_fold]\n",
        "    iteration_k_fold +=1\n",
        "\n",
        "    # Select dataset rows for the iteration.\n",
        "    X_test=dataset_iteration.loc[test_set]\n",
        "    # Select labels for the iteration.\n",
        "    y_test=[y_forum[j] for j in test_set]\n",
        "\n",
        "    for name,model in models.items():\n",
        "      print(\"Predicting: {}, dataset {}, iteration {}\".format(name,dataset,str(iteration_k_fold)))\n",
        "      scores_p[name]=(precision_score(y_test,models[name].predict(X_test),average='weighted',zero_division=0)/5)+scores_p[name]\n",
        "      scores_r[name]=(recall_score(y_test,models[name].predict(X_test),average='weighted',zero_division=0)/5)+scores_r[name]\n",
        "      scores_f[name]=(f1_score(y_test,models[name].predict(X_test),average='weighted',zero_division=0)/5)+scores_f[name]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting: knn, dataset vector, iteration 1\n",
            "Predicting: tree, dataset vector, iteration 1\n",
            "Predicting: svm, dataset vector, iteration 1\n",
            "Predicting: knn, dataset vector, iteration 2\n",
            "Predicting: tree, dataset vector, iteration 2\n",
            "Predicting: svm, dataset vector, iteration 2\n",
            "Predicting: knn, dataset vector, iteration 3\n",
            "Predicting: tree, dataset vector, iteration 3\n",
            "Predicting: svm, dataset vector, iteration 3\n",
            "Predicting: knn, dataset vector, iteration 4\n",
            "Predicting: tree, dataset vector, iteration 4\n",
            "Predicting: svm, dataset vector, iteration 4\n",
            "Predicting: knn, dataset vector, iteration 5\n",
            "Predicting: tree, dataset vector, iteration 5\n",
            "Predicting: svm, dataset vector, iteration 5\n",
            "Predicting: knn, dataset embedding_response, iteration 1\n",
            "Predicting: tree, dataset embedding_response, iteration 1\n",
            "Predicting: svm, dataset embedding_response, iteration 1\n",
            "Predicting: knn, dataset embedding_response, iteration 2\n",
            "Predicting: tree, dataset embedding_response, iteration 2\n",
            "Predicting: svm, dataset embedding_response, iteration 2\n",
            "Predicting: knn, dataset embedding_response, iteration 3\n",
            "Predicting: tree, dataset embedding_response, iteration 3\n",
            "Predicting: svm, dataset embedding_response, iteration 3\n",
            "Predicting: knn, dataset embedding_response, iteration 4\n",
            "Predicting: tree, dataset embedding_response, iteration 4\n",
            "Predicting: svm, dataset embedding_response, iteration 4\n",
            "Predicting: knn, dataset embedding_response, iteration 5\n",
            "Predicting: tree, dataset embedding_response, iteration 5\n",
            "Predicting: svm, dataset embedding_response, iteration 5\n",
            "Predicting: knn, dataset embedding_original, iteration 1\n",
            "Predicting: tree, dataset embedding_original, iteration 1\n",
            "Predicting: svm, dataset embedding_original, iteration 1\n",
            "Predicting: knn, dataset embedding_original, iteration 2\n",
            "Predicting: tree, dataset embedding_original, iteration 2\n",
            "Predicting: svm, dataset embedding_original, iteration 2\n",
            "Predicting: knn, dataset embedding_original, iteration 3\n",
            "Predicting: tree, dataset embedding_original, iteration 3\n",
            "Predicting: svm, dataset embedding_original, iteration 3\n",
            "Predicting: knn, dataset embedding_original, iteration 4\n",
            "Predicting: tree, dataset embedding_original, iteration 4\n",
            "Predicting: svm, dataset embedding_original, iteration 4\n",
            "Predicting: knn, dataset embedding_original, iteration 5\n",
            "Predicting: tree, dataset embedding_original, iteration 5\n",
            "Predicting: svm, dataset embedding_original, iteration 5\n",
            "Predicting: knn, dataset combined, iteration 1\n",
            "Predicting: tree, dataset combined, iteration 1\n",
            "Predicting: svm, dataset combined, iteration 1\n",
            "Predicting: knn, dataset combined, iteration 2\n",
            "Predicting: tree, dataset combined, iteration 2\n",
            "Predicting: svm, dataset combined, iteration 2\n",
            "Predicting: knn, dataset combined, iteration 3\n",
            "Predicting: tree, dataset combined, iteration 3\n",
            "Predicting: svm, dataset combined, iteration 3\n",
            "Predicting: knn, dataset combined, iteration 4\n",
            "Predicting: tree, dataset combined, iteration 4\n",
            "Predicting: svm, dataset combined, iteration 4\n",
            "Predicting: knn, dataset combined, iteration 5\n",
            "Predicting: tree, dataset combined, iteration 5\n",
            "Predicting: svm, dataset combined, iteration 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of4tyPrYggCQ",
        "outputId": "d6567081-1317-4a29-a76f-d47cfb0ee240"
      },
      "source": [
        "scores_names=['precision', 'recall', 'f1-score']\n",
        "scores_dict=[precision_scores, recall_scores, f1_scores]\n",
        "\n",
        "for i in range(3):\n",
        "  print('\\n{}:\\n'.format(scores_names[i]))\n",
        "\n",
        "  print('{:<25} {:<15} {:<15}'.format('dataset' ,'model', 'score'))\n",
        "\n",
        "  for d_names,dataset_scores in scores_dict[i].items():\n",
        "    for name, value in dataset_scores.items():\n",
        "      print('{:<25} {:<15} {:<15}'.format(d_names ,name, round(value,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "precision:\n",
            "\n",
            "dataset                   model           score          \n",
            "vector                    knn             0.34           \n",
            "vector                    tree            0.34           \n",
            "vector                    svm             0.31           \n",
            "embedding_response        knn             0.36           \n",
            "embedding_response        tree            0.32           \n",
            "embedding_response        svm             0.34           \n",
            "embedding_original        knn             0.36           \n",
            "embedding_original        tree            0.32           \n",
            "embedding_original        svm             0.34           \n",
            "combined                  knn             0.37           \n",
            "combined                  tree            0.33           \n",
            "combined                  svm             0.32           \n",
            "\n",
            "recall:\n",
            "\n",
            "dataset                   model           score          \n",
            "vector                    knn             0.44           \n",
            "vector                    tree            0.34           \n",
            "vector                    svm             0.45           \n",
            "embedding_response        knn             0.43           \n",
            "embedding_response        tree            0.3            \n",
            "embedding_response        svm             0.46           \n",
            "embedding_original        knn             0.43           \n",
            "embedding_original        tree            0.29           \n",
            "embedding_original        svm             0.47           \n",
            "combined                  knn             0.43           \n",
            "combined                  tree            0.32           \n",
            "combined                  svm             0.46           \n",
            "\n",
            "f1-score:\n",
            "\n",
            "dataset                   model           score          \n",
            "vector                    knn             0.34           \n",
            "vector                    tree            0.33           \n",
            "vector                    svm             0.34           \n",
            "embedding_response        knn             0.33           \n",
            "embedding_response        tree            0.31           \n",
            "embedding_response        svm             0.38           \n",
            "embedding_original        knn             0.33           \n",
            "embedding_original        tree            0.3            \n",
            "embedding_original        svm             0.37           \n",
            "combined                  knn             0.33           \n",
            "combined                  tree            0.32           \n",
            "combined                  svm             0.37           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0KCTvtUggeT"
      },
      "source": [
        "**Final comparison, new model and new test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-31R9rcYgss7"
      },
      "source": [
        "# A new dataset composed by embedding of the disagreement messages and the messages they referred to is created\n",
        "# for the chat dataset.\n",
        "\n",
        "embedding_chat=[]\n",
        "documents_vector_rapresentation=embed_corpus(corpus_chat)\n",
        "\n",
        "# Lists of the couples of embedding (200 response messages).\n",
        "for i in range(200):\n",
        "  next_vector=np.concatenate((documents_vector_rapresentation[i*2],documents_vector_rapresentation[i*2+1]),axis=None)\n",
        "  embedding_chat.append(next_vector)\n",
        "\n",
        "X_embedding_chat=pd.DataFrame(embedding_chat)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0ZP1P3zt0TH"
      },
      "source": [
        "# Separate the training and the test set for the forum dataset (800-200 rispectively).\n",
        "\n",
        "# Train set.\n",
        "X_train_forum=X_embedding_2_forum_dataset.loc[0:799]\n",
        "y_train_forum=y_forum[0:800]\n",
        "\n",
        "# Test set.\n",
        "X_test_forum=X_embedding_2_forum_dataset.loc[800:999]\n",
        "y_test_forum=y_forum[800:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snThG0Emto_c"
      },
      "source": [
        "# Build a single model that uses the support vector machine classifier.\n",
        "smv_model=SVC().fit(X_train_forum,y_train_forum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88npzro45Uv6",
        "outputId": "1a98dba8-090b-4e5a-beae-9577f414b33f"
      },
      "source": [
        "# Report of the results of the test on the forum's texts.\n",
        "print(classification_report(y_test_forum, smv_model.predict(X_test_forum),zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.25      0.08      0.12        13\n",
            "           3       0.38      0.37      0.37        57\n",
            "           4       0.61      0.87      0.72        99\n",
            "           5       0.00      0.00      0.00        12\n",
            "           6       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.54       200\n",
            "   macro avg       0.18      0.19      0.17       200\n",
            "weighted avg       0.43      0.54      0.47       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoIk_C-u4tT_",
        "outputId": "009b9483-a076-421f-d863-945c18d4986a"
      },
      "source": [
        "# Report of the results of the test on the chat's texts.\n",
        "print(classification_report(y_chat, smv_model.predict(X_embedding_chat), labels=[0,1,2,3,4,5,6],zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.40      0.94      0.56        68\n",
            "           4       0.82      0.28      0.42       110\n",
            "           5       0.00      0.00      0.00        11\n",
            "           6       0.00      0.00      0.00         6\n",
            "\n",
            "   micro avg       0.47      0.47      0.48       200\n",
            "   macro avg       0.17      0.17      0.14       200\n",
            "weighted avg       0.59      0.47      0.42       200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POiJVnEl4UMp"
      },
      "source": [
        "# Compute the confusion matrices."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "oQEf--Wr5Mlt",
        "outputId": "551d9fea-2ee7-4268-a246-4135fcf52cf4"
      },
      "source": [
        "# forum confusion matrix.\n",
        "plot_confusion_matrix(smv_model, X_test_forum, y_test_forum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb774ce4ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "8Zoc64mh5hBu",
        "outputId": "947d0cca-b38d-421c-9c37-4f8ebe4703df"
      },
      "source": [
        "# chat confusion matrix.\n",
        "plot_confusion_matrix(smv_model, X_embedding_chat, y_chat, labels=[0,1,2,3,4,5,6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb774c50250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}